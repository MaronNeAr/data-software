## 第一部分 数据系统基础

#### 可靠、可扩展与可维护的应用系统

###### 可靠性（Reliability）

- **MTTF（Mean Time To Failure）**：平均无故障时间，衡量系统稳定性。
- **MTTR（Mean Time To Recovery）**：平均恢复时间，反映故障修复效率。
- **目标**：通过设计降低 MTTR（快速恢复）比单纯延长 MTTF（减少故障）更重要。

###### 可扩展性（Scalability）

- **垂直扩展（Scale Up）**：提升单机性能（如升级 CPU、内存），但受硬件上限限制。
- **水平扩展（Scale Out）**
  - 增加机器数量，通过分布式架构分担负载。
  - **挑战**：数据分片、负载均衡、一致性协调。
- **弹性扩展（Auto-Scaling）**：根据负载动态调整资源（如 AWS Auto Scaling 组）。

###### 可维护性（Maintainability）

- **可操作性（Operability）**：便于运维团队监控、诊断和管理系统。
  - 提供监控仪表盘（如 Prometheus + Grafana）。
  - 自动化运维任务（如日志轮转、备份）。
- **简单性（Simplicity）**：通过抽象减少复杂性，避免“屎山代码”。
  - 模块化设计（如微服务拆分）。
  - 使用声明式配置（如 Kubernetes YAML）。
- **可演化性（Evolvability）**：系统能够适应未来的需求变化（如新增功能、架构升级）。
  - 向后兼容的 API 设计（如 GraphQL 的灵活查询）。
  - 松耦合的组件交互（如消息队列解耦生产者和消费者）。

#### 数据模型与查询语言

###### 数据模型的核心作用

- **存储效率**：如何组织数据以节省空间或提高访问速度。
- **查询能力**：支持何种查询模式（如连接、聚合、图遍历）。
- **演化灵活性**：是否容易适应需求变化（如新增字段、调整关系）。

###### 关系模型（Relational Model）

- **核心思想**

  - **表结构**：数据以二维表（Table）形式存储，每行表示一个实体，每列表示属性。

  - **范式化（Normalization）**：通过分解表减少冗余（如将订单和商品拆分为多张表）。

- **查询语言：SQL**

  - **声明式语言**：用户指定“需要什么”，而非“如何获取”（如 `SELECT * FROM users WHERE age > 30`）。

  - 支持复杂查询（多表连接、子查询、聚合函数）。
  - 事务保证（ACID）。

- **局限**：处理嵌套数据（如JSON数组）较繁琐；水平扩展困难（分布式关系型数据库复杂度高）。

- **适用场景**：事务处理（OLTP）：如银行系统、订单管理；需要强一致性和复杂查询的业务。

###### 文档模型（Document Model）

- **核心思想**

  - **半结构化数据**：以文档（如JSON、XML）形式存储数据，允许嵌套结构。

  - **反范式化（Denormalization）**：将关联数据嵌入同一文档（如用户信息与其订单记录合并存储）。

- **适用场景**：内容管理系统（CMS）：如博客、产品目录；事件日志、实时分析（如存储传感器数据）。

###### 图模型（Graph Model）

- **核心思想**

  - **以关系为中心**：数据由节点（Node）和边（Edge）构成，直接表示实体间的复杂关系。

  - **示例场景**：社交网络（用户为节点，关注关系为边）、推荐系统（商品与用户的交互关系）。

- **优势**：高效处理多跳查询（如“查找朋友的朋友”）；直观表示复杂关系网络。

- **局限**：不适合简单结构化数据（如订单管理）；分布式图数据库的扩展性挑战。

#### 数据存储与检索

###### 存储引擎的核心目标

- **写入性能**：数据写入的速度（如高吞吐日志写入）。
- **读取性能**：随机读取与范围查询的效率。
- **存储效率**：磁盘空间利用率与数据压缩能力。
- **持久性**：数据在崩溃或断电后的恢复能力。

###### 日志结构存储引擎（LSM-Tree）

- **追加写（Append-Only）**：所有写入以日志形式追加到文件末尾，避免随机写磁盘。
- **合并与压缩（Compaction）**：定期将多个小文件合并为大文件，并清理过期数据。
- **MemTable**：内存中有序数据结构（跳表）缓存近期写入的数据；写满后冻结，转为不可变的 SSTable（Sorted String Table）。
- **SSTable**：磁盘上的有序文件，按键排序存储数据；支持快速范围查询（通过二分查找或稀疏索引）。
- **Compaction 过程**：合并多个 SSTable，删除重复和过期数据（如 LevelDB 的分层合并策略）。
- **高写入吞吐**：顺序写磁盘，适合写密集型场景（如日志、时序数据）。
- **自动压缩**：减少碎片化，优化存储空间。
- **读放大（Read Amplification）**：查询可能需要多次磁盘访问（MemTable + 多层 SSTable）。
- **写放大（Write Amplification）**：合并操作可能重复写入数据。

###### 页式存储引擎（B-Tree）

- **核心思想**
  - **固定大小的页（Page）**：数据按页（如 4KB）组织，通过 B-Tree 结构管理。
  
  - **就地更新（In-Place Update）**：直接修改磁盘上的页内容。
  
- **B-Tree 结构**：平衡多路搜索树，每个节点包含多个键和子节点指针；叶子节点存储实际数据或数据指针。

- **写前日志（WAL）**：所有修改先写入日志（如 InnoDB 的 redo log），确保崩溃恢复。
- **页分裂与合并**：插入数据导致页溢出时分裂为两页；删除数据后若页空间过少，则合并相邻页。

###### LSM-Tree vs. B-Tree 的对比

| **特性**     | **LSM-Tree**               | **B-Tree**                 |
| :----------- | :------------------------- | :------------------------- |
| **写入性能** | 高（顺序追加）             | 低（随机写）               |
| **读取性能** | 范围查询快，随机读可能较慢 | 随机读快                   |
| **存储空间** | 压缩后更高效，但有写放大   | 可能碎片化，需定期维护     |
| **适用场景** | 写密集、分析型负载（OLAP） | 读密集、事务型负载（OLTP） |

###### 列式存储（Column-Oriented Storage）

- **核心思想**
  - **按列存储数据**：将同一列的数据连续存储（如 Parquet、ORC 格式）。
  
  - **优化分析查询**：适合统计某列的聚合值（SUM、AVG）。
  
- **字典编码**：用整数代替重复值（如将国家名映射为 ID）。
- **位图索引**：高效表示稀疏数据（如性别列只有 Male/Female）。
- **向量化处理**：对整列数据批量操作（SIMD 指令加速计算）。
- **延迟物化（Late Materialization）**：在查询后期才拼接多列数据，减少中间数据传输。

###### 数据仓库优化技术

- **分区（Partitioning）**：按时间或键范围分区，加速范围查询（如按日期分区日志数据）。
- **物化视图（Materialized View）**：预计算聚合结果（如每日销售额），避免实时计算。
- **索引优化**：
  - **位图索引**：适合低基数列（如性别、状态）。
  - **布隆过滤器（Bloom Filter）**：快速判断某键是否存在，减少磁盘访问。

#### 数据编码与演化

###### 文本格式（人类可读）

- **JSON**：
  - **优点**：广泛支持、易调试、适合 Web API。
  - **缺点**：冗余度高（字段名重复）、无严格模式约束。
  - **兼容性**：宽松模式，但需手动处理字段缺失或新增。
- **XML**：
  - **优点**：支持复杂结构和命名空间。
  - **缺点**：冗长、解析性能差。

###### 二进制格式（高效紧凑）

- **Protocol Buffers（Protobuf）**
  - **模式驱动**：需预定义 `.proto` 文件，生成代码。
  - **特点**：字段通过编号（Tag）标识，可省略字段名；支持字段的**可选性**（optional/repeated）。
  - **向后兼容**：新代码读取旧数据时，忽略未知字段。
  - **向前兼容**：旧代码读取新数据时，保留未知字段不解析。
- **Apache Avro**
  - **动态模式**：写入和读取需显式指定模式（Schema）。
  - **特点**：数据中不存储字段名，仅按模式定义的顺序编码；依赖“写入模式”与“读取模式”的**模式解析规则**。
  - **兼容性策略**：字段可通过别名映射，支持字段类型演进（如 int → long）。
- **Thrift**：类似 Protobuf，但提供更多传输协议选项（如二进制、压缩格式）。

###### 模式演进与兼容性

- **向后兼容（Backward Compatibility）**

  - **定义**：新代码能够读取旧数据。

  - **实现方式**：新增字段设为**可选**（optional）；禁止删除已使用的字段或修改字段类型（如 int → string）。

- **向前兼容（Forward Compatibility）**

  - **定义**：旧代码能够读取新数据。
  - **实现方式**：旧代码忽略新字段（如 Protobuf 的未知字段保留）；避免修改字段的语义或必填性。

- **模式演进规则**

  - **新增字段**：需设为可选，并设置默认值（如 Protobuf 的 `optional`）。
  - **删除字段**：标记为弃用，保留字段编号避免重用。
  - **重命名字段**：通过别名映射（Avro）或保留旧编号（Protobuf）。

  - **类型变更**：仅允许兼容类型扩展（如 int32 → int64）。

## 第二部分 分布式数据系统

#### 数据复制

###### 主从复制（Leader-Follower Replication）

- **主节点（Leader）**：处理所有写请求，并将变更同步到从节点。
- **从节点（Follower）**：仅处理读请求，异步或同步复制主节点数据。
- **全同步**：主节点等待所有从节点确认写入后才返回成功（强一致，高延迟）。
- **半同步**：主节点等待至少一个从节点确认（平衡一致性与延迟）。
- **异步**：主节点写入本地后立即返回，从节点异步追赶（低延迟，可能丢数据）。

###### 多主复制（Multi-Leader Replication）

- **多主节点**：每个主节点均可处理读写请求，并相互同步数据。
- **冲突解决**：需处理多主并发写入冲突（如最后写入胜出、自定义合并逻辑）。
- **多地数据中心**：每个数据中心部署一个主节点，减少跨地域延迟。
- **离线优先应用**：设备本地写入（如移动端），网络恢复后同步到云端。

###### 无主复制（Leaderless Replication）

- **去中心化**：所有节点均可处理读写请求，无主从角色区分。
- **读写仲裁**：通过`W + R > N`策略保证一致性（N为副本总数，W/R为读写确认数）。
- **写入**：客户端并发写入多个节点（至少W个成功）。
- **读取**：客户端并发读取多个节点（至少R个响应），按版本号合并最新值。
- **修复**：后台进程检测副本差异并修复（反熵过程）。

###### 复制中的一致性问题

- **同步复制**：强一致性，但延迟高（等待所有副本确认）。
- **异步复制**：最终一致性，低延迟，但可能丢数据（主节点宕机前未同步）。
- **读己之写（Read-After-Write）不一致**：
  - 用户写入后立即读取，可能读到旧副本。
  - 解决方案：写后读主节点（牺牲读扩展性）；跟踪复制进度，仅读取已同步的副本。
- **单调读（Monotonic Reads）保证**：
  - 确保用户不会看到数据“回退”（如先读到新值，再读到旧值）。
  - 解决方案：同一用户的读请求路由到同一副本。

###### 复制冲突与解决策略

- **冲突类型**

  - **写-写冲突**：多个节点并发修改同一数据（如多主/无主模型）。

  - **读-写冲突**：读取时数据正在复制，导致结果不一致。

- **冲突解决机制**

  - **Last-Write-Wins（LWW）**：基于时间戳选择最新写入，简单但可能丢数据（时钟偏差问题）。
  - **客户端解决**：写入时携带业务逻辑合并冲突（如购物车合并商品）。
  - **CRDTs（Conflict-Free Replicated Data Types）**：设计数据结构自动收敛（如计数器、集合）。
  - **操作转换（OT）**：协同编辑场景下，对操作顺序进行变换（如Google Docs）。

#### 数据分片

###### 水平分片（Horizontal Sharding）

- **定义**：按行（记录）划分数据，不同分片存储不同的行子集。
- **分片键（Shard Key）**：用于划分数据的字段（如用户ID、时间戳）。
- **分片策略**
  - **范围分片（Range Sharding）**：按分片键的值范围划分（如用户ID 1-1000在分片1，1001-2000在分片2）。
  - **哈希分片（Hash Sharding）**：对分片键哈希取模，映射到不同分片（如`hash(user_id) % 3`）。
  - **列表分片（List Sharding）**：按分片键的枚举值分配（如地区字段值为“北京”的分片1，“上海”的分片2）。

###### 垂直分片（Vertical Sharding）

- **定义**：按列划分数据，不同分片存储不同的列子集。
- **优点**：减少单表宽度，提升高频字段查询性能。
- **缺点**：查询需跨分片关联（如`JOIN`操作），复杂度高。

###### 混合分片（Hybrid Sharding）

- **定义**：组合水平与垂直分片，先垂直拆分再水平扩展。

###### 分片键（Shard Key）的选择原则

- **高基数**：分片键的值应分布均匀（如用户ID而非性别）。
- **业务相关性**：分片键应匹配高频查询条件（如按订单时间分片支持时间范围查询）。
- **避免热点**：避免选择单调递增的键（如自增ID），导致写入集中在最新分片。
- **稳定性**：分片键的值应尽量不变（如用户ID而非手机号）。

###### 数据路由机制

- **客户端路由**：客户端根据分片规则直接连接目标分片。
- **代理层路由**：通过代理（如MySQL Router、ShardingSphere-Proxy）解析SQL并路由到对应分片。
- **协调节点路由**：由分布式数据库的协调节点（如CockroachDB的Gateway Node）统一管理路由。

#### 一致性与共识

###### CAP理论

- **CAP三要素的定义**

  | **要素**                              | **定义**                                                     |
  | :------------------------------------ | :----------------------------------------------------------- |
  | **一致性（Consistency）**             | 所有节点在同一时刻看到的数据完全相同（强一致性）。           |
  | **可用性（Availability）**            | 每个请求都能在合理时间内获得非错误响应（即使部分节点故障）。 |
  | **分区容忍性（Partition Tolerance）** | 系统在网络分区（节点间通信中断）时仍能继续运行。             |

- **CP系统（放弃A）**：

  - **特点**：在网络分区时，牺牲可用性以保持一致性。
  - **示例**：HBase、ZooKeeper、传统关系型数据库集群。
  - **场景**：金融交易、计费系统等强一致性需求场景。

- **AP系统（放弃C）**：

  - **特点**：在网络分区时，牺牲一致性以保持可用性，采用最终一致性。
  - **示例**：Cassandra、DynamoDB、CouchDB。
  - **场景**：社交媒体、实时日志处理等高可用需求场景。

- **CA系统（放弃P）**：仅在无网络分区时同时满足C和A，实际中无法实现（网络分区不可避免）。

###### 一致性模型

- **强一致性（Strong Consistency）**

  - **定义**：所有读写操作表现为原子性，后续操作总能读取到最新写入的值。

  - **实现方式**：同步复制（如2PC协议）；全局锁或共识算法（如Paxos、Raft）。

- **最终一致性（Eventual Consistency）**

  - **定义**：若无新写入，所有节点最终会达成一致状态，允许中间状态不一致。

  - **实现方式**：异步复制（如Gossip协议）；冲突解决策略（如Last-Write-Win、向量时钟）。

- **因果一致性（Causal Consistency）**

  - **定义**：保证存在因果关系的操作顺序一致，无关操作可以乱序。

  - **实现方式**：跟踪操作依赖关系（如向量时间戳）。

- **会话一致性（Session Consistency）**

  - **定义**：同一会话内保证读写一致性，跨会话允许弱一致性。

  - **实现方式**：客户端会话绑定到固定节点（如Sticky Session）。

- **单调读一致性（Monotonic Reads）**

  - **定义**：同一客户端不会读到比之前更旧的数据。

  - **实现方式**：客户端记录最新版本号，读取时至少不低于该版本。

- **线性一致性（Linearizability）**：所有操作按全局顺序执行，且实时生效（强一致性的子集）。

###### Paxos算法

- **角色定义**

  - **Proposer**：提出提案（如写入请求）。

  - **Acceptor**：接受或拒绝提案，存储已接受的提案。

  - **Learner**：学习最终被选中的提案（可选角色）。

- **阶段1：Prepare（准备）**

  - **Proposer生成提案号**：选择一个全局唯一的递增编号`n`（如时间戳+节点ID）。

  - **广播Prepare请求**：向所有Acceptor发送`Prepare(n)`。

  - **Acceptor响应**：若`n`是收到过的最大的提案号，则返回已接受的最高编号提案（若有），并承诺不再接受比`n`小的提案，否则拒绝请求。

- **阶段2：Accept（接受）**

  - **Proposer选择提案值**：若收到多数派Acceptor的响应，选择响应中最高编号的提案值（若存在），否则使用自己的值。

  - **广播Accept请求**：发送`Accept(n, value)`。

  - **Acceptor接受提案**：若未承诺过更大的提案号，则接受`(n, value)`并持久化。

  - **Learner学习最终值**：当多数派Acceptor接受某提案时，该值被选定。

###### Raft算法

- **角色定义**

  - **Leader**：唯一处理客户端请求的节点，管理日志复制。

  - **Follower**：被动接收Leader的日志条目，参与选举。

  - **Candidate**：竞选Leader的临时状态。

- **Leader选举**

  - **超时触发选举**：Follower在选举超时（Election Timeout）未收到Leader心跳后，成为Candidate。

  - **发起投票请求**：Candidate向所有节点发送`RequestVote`请求。

  - **投票规则**：每个节点在一个任期内只能投一票；Candidate需获得多数派投票才能成为Leader。

  - **新Leader产生**：新Leader定期发送心跳（`AppendEntries`）维持权威。

- **日志复制**

  - **客户端请求处理**：Leader将请求追加到本地日志，标记为未提交（Uncommitted）。

  - **广播日志条目**：Leader向所有Follower发送`AppendEntries`请求。

  - **Follower确认**：Follower验证日志一致性后，追加条目并回复确认。

  - **提交日志**：当多数派Follower确认后，Leader提交日志（Committed），并通知Follower提交。

- **安全性保证**

  - **选举限制**：只有拥有最新日志的Candidate才能成为Leader（防止数据回滚）。

  - **日志匹配**：Leader强制覆盖Follower的不一致日志。

## 第三部分 批处理系统

#### MapReduce：批处理的基石

###### MapReduce 的核心思想

- **Map 阶段**：将输入数据拆分为多个分片（Split），并行处理生成中间键值对（Key-Value Pairs）。

- **Shuffle 阶段**：隐式阶段，负责将 Map 输出排序、分组并传输到 Reduce 节点。

- **Reduce 阶段**：对中间结果按 Key 分组，进行聚合或转换，输出最终结果。

- **编程模型**

  ```python
  # Map 函数：处理输入，生成中间键值对
  def map(key, value):
      for item in process(value):
          yield (intermediate_key, intermediate_value)
  # Reduce 函数：聚合相同 Key 的值
  def reduce(key, values):
      result = aggregate(values)
      yield (key, result)
  ```

###### MapReduce 执行流程

- **输入分片（Input Splits）**

  - 数据存储于分布式文件系统（如 HDFS），按固定大小（如 128MB）分片。

  - 每个分片启动一个 **Map 任务**，由框架自动调度到集群节点。

- **Map 阶段**
  - **读取输入**：从 HDFS 读取分片数据（如文本文件的一行）。
  - **执行 Map 函数**：用户自定义逻辑处理数据，输出中间键值对。
  - **写入本地磁盘**：Map 结果按 Key 分区（Partition）后写入节点本地磁盘（非 HDFS）。

- **Shuffle 与排序**

  - **Fetch 阶段**：Reduce 任务从所有 Map 节点拉取对应分区的数据。

  - **排序与合并**：拉取的数据按 Key 排序，合并相同 Key 的值列表。

- **Reduce 阶段**
  - **执行 Reduce 函数**：对排序后的键值对进行聚合（如求和、去重）。
  - **输出结果**：结果写入 HDFS，每个 Reduce 任务生成一个输出文件。

###### MapReduce 的关键机制

- **容错与恢复**

  - **Task 失败**：

    > Map/Reduce 任务失败时，框架重新调度到其他节点执行
    >
    > 已完成的 Map 任务需重新执行（因中间结果存储在失败节点的本地磁盘）。

  - **节点故障**：通过心跳检测（Heartbeat）发现宕机节点，重新分配其任务。

- **数据本地性优化**

  - 将 Map 任务调度到存储输入分片的节点，避免跨网络读取数据。
  - 若本地节点繁忙，选择同一机架内的节点（机架感知策略）。

- **Combiner 优化**

  - **局部聚合**：在 Map 阶段后、Shuffle 前，对中间结果进行预聚合（类似 Reduce）。
  - **减少数据传输**：例如 WordCount 中，Map 节点先合并相同单词的计数。

#### 现代批处理框架

###### Apache Spark

- **内存计算**：
  - 将中间数据缓存到内存（RDD），减少磁盘I/O。
  - 相比MapReduce，迭代算法性能提升10-100倍。
- **DAG执行引擎**：
  - 将作业拆分为有向无环图（DAG），优化任务调度。
  - 支持多阶段任务合并（如Map后直接Reduce，跳过Shuffle）。
- **统一API**：支持批处理（Spark Core）、流处理（Spark Streaming）、机器学习（MLlib）。

###### 执行引擎优化

- **向量化处理**：使用SIMD指令批量处理数据（如Apache Arrow内存格式）。
- **动态资源分配**：根据任务负载动态调整CPU/内存（如YARN或Kubernetes调度器）。

###### 存储优化：列式存储

- **Parquet/ORC格式**：
  - 按列存储，高压缩率（同列数据类型一致）。
  - 支持谓词下推（Predicate Pushdown），仅读取需要的列。
- **数据分区分桶**：按时间或哈希分区，加速过滤和聚合。

#### 批处理系统架构

###### 数据存储层

- **分布式文件系统**
  - **HDFS（Hadoop Distributed File System）**：分块存储（128MB/块），多副本冗余；适合存储大文件，如日志、原始数据。
  - **云存储（S3、GCS）**：对象存储服务，弹性扩展，按需付费。
- **数据湖（Data Lake）**
  - **原始数据存储**：支持结构化（CSV）、半结构化（JSON）、非结构化（图片）数据。
  - **列式存储格式**：Parquet-高压缩率，适合OLAP查询；ORC-优化Hive查询性能，支持谓词下推。
- **数据仓库（Data Warehouse）**
  - **结构化存储**：基于星型/雪花模型，优化聚合查询。
  - **代表系统**：Snowflake、Redshift（云原生数仓）；Hive（基于HDFS的SQL查询引擎）。

###### 计算引擎层

- **MapReduce（Hadoop）**
  - **经典批处理模型**：分Map、Shuffle、Reduce三阶段。
  - **适用场景**：简单ETL、离线分析。
- **Apache Spark**
  - **内存计算**：通过RDD（弹性分布式数据集）减少磁盘I/O。
  - **DAG调度**：优化任务依赖，支持多阶段流水线执行。
  - **统一API**：支持批处理（Spark SQL）、流处理（Structured Streaming）、机器学习（MLlib）。
- **Apache Flink（批处理模式）**
  - **流批一体**：同一API处理批和流数据（DataSet API）。
  - **增量计算**：优化迭代任务（如图计算、机器学习）。
- **Presto/Trino**
  - **分布式SQL引擎**：直接查询数据湖（如Hive表、S3文件）。
  - **联邦查询**：跨数据源联合分析（如MySQL + Hive）。

###### 资源管理层

- **Hadoop YARN**

  - **资源调度**：将集群资源划分为容器（Container），分配给MapReduce、Spark等任务。
  - **队列管理**：支持多租户资源隔离（如生产队列 vs. 实验队列）。

- **Kubernetes**

  - **容器化部署**：将批处理作业封装为Pod，动态扩缩容。

  - **Operator模式**：

    > **Spark Operator**：在K8s上原生运行Spark作业。
    >
    > **Flink Kubernetes Session**：管理Flink集群生命周期。

###### 工作流调度层

- **Apache Airflow**
  - **DAG定义**：通过Python代码定义任务依赖关系。
  - **监控与重试**：可视化任务状态，自动重试失败任务。
- **Oozie（Hadoop生态）**
  - **XML配置**：定义MapReduce、Hive、Sqoop任务流。
  - **协调器（Coordinator）**：定时触发工作流（如每天凌晨执行ETL）。

###### 服务层

- **数据服务API**
  - **RESTful API**：通过HTTP接口暴露聚合结果（如统计报表）。
  - **GraphQL**：灵活查询数据湖中的多源数据。
- **BI与可视化工具**
  - **Tableau/Power BI**：连接数据仓库，生成交互式仪表盘。
  - **Superset**：开源BI工具，支持SQL Lab直接查询。
- **机器学习模型服务**
  - **批量预测**：定期生成用户推荐列表、风险评分。
  - **模型更新**：每天训练新模型并发布到生产环境。

#### 批处理应用场景

###### ETL（Extract, Transform, Load）

- **数据清洗**：过滤无效记录（如空值、异常值）、标准化格式（日期、货币单位）。
- **数据转换**：合并多源数据（如用户信息与订单记录关联）、计算衍生字段（如用户年龄=当前日期-出生日期）。
- **数据加载**：将处理后的数据写入目标存储（如Hive表、Snowflake数仓）。

###### 离线机器学习

- **特征工程**：批量生成特征（如用户过去30天的购买总额）。
- **模型训练**：在分布式集群上训练分类、回归或聚类模型。
- **离线评估**：计算模型指标（如准确率、AUC）。

###### 数据仓库构建

- **贴源层（ODS，Operational Data Store）**
  - **全量/增量同步**：通过ETL工具从业务库（MySQL、Oracle）实时或定期抽取数据。
  - **数据原样存储**：保留原始格式（如JSON日志、数据库表结构），不做业务逻辑处理。
  - **短期存储**：通常保留最近7~30天的数据，供问题回溯使用。
- **明细层（DWD，Data Warehouse Detail）**
  - **数据清洗**：处理缺失值、去重、统一格式（如时间戳标准化）。
  - **维度退化**：将多张表关联为宽表（如订单表+商品表→订单明细宽表）。
  - **业务逻辑解耦**：屏蔽底层业务系统的表结构差异（如不同系统的用户ID映射）。

- **汇总层（DWS，Data Warehouse Summary）**
  - **维度聚合**：按时间、地域、产品等维度统计指标（如每日销售额、用户活跃数）。
  - **主题域划分**：按业务主题（如交易、流量、用户）组织数据。
  - **中间结果复用**：避免重复计算，加速上层应用查询。

- **应用层（ADS，Application Data Service）**
  - **业务指标封装**：生成报表、BI看板所需的数据（如GMV、转化率）。
  - **跨主题整合**：融合多个主题数据（如用户画像+交易数据→高价值用户列表）。
  - **数据服务化**：通过API或数据接口暴露给业务系统（如推荐系统、风控系统）。

- **维度层（DIM，Dimension）**
  - **维度一致性**：统一各层的维度定义（如地区、时间、产品分类）。
  - **缓慢变化维（SCD）管理**：处理维度属性变化（如用户地址变更）。
  - **字典表存储**：码值映射表（如状态码→中文描述）。

#### 批处理系统优化

###### 计算性能优化

| **优化方向**     | **具体措施**                              | **案例/工具**                      |
| :--------------- | :---------------------------------------- | :--------------------------------- |
| **内存计算**     | 将中间数据缓存至内存，减少磁盘I/O。       | Spark RDD缓存、Flink托管内存       |
| **向量化执行**   | 使用SIMD指令批量处理数据，提升CPU利用率。 | Apache Arrow、Presto向量化引擎     |
| **动态代码生成** | 运行时生成优化代码，避免解释执行开销。    | Spark Catalyst优化器、LLVM编译     |
| **数据本地性**   | 调度任务到数据所在节点，减少网络传输。    | Hadoop机架感知、Kubernetes拓扑调度 |

###### 数据倾斜处理

| **优化方向**        | **具体措施**                                          | **适用场景**                 |
| :------------------ | :---------------------------------------------------- | :--------------------------- |
| **Salting（加盐）** | 为Key添加随机前缀，将热点数据分散到多个分区。         | 大Key聚合（如用户ID热点）    |
| **两阶段聚合**      | 先局部聚合（Map端Combiner），再全局聚合（Reduce端）。 | 高基数Key统计（如PV计数）    |
| **动态分区调整**    | 根据数据分布自动调整分区策略（如范围分区→哈希分区）。 | Spark自适应查询（AQE）       |
| **倾斜Key隔离**     | 识别热点Key单独处理，其余正常计算。                   | 电商大促期间头部商品订单分析 |

###### 容错与一致性保障

| **优化方向**     | **具体措施**                                   | **技术实现**                       |
| :--------------- | :--------------------------------------------- | :--------------------------------- |
| **检查点机制**   | 定期保存任务状态，故障时从检查点恢复。         | Spark Checkpoint、Flink Savepoints |
| **幂等性设计**   | 确保任务重试不会导致重复结果。                 | 数据库UPSERT、Kafka生产者幂等配置  |
| **输出提交协议** | 仅在所有任务成功后提交最终结果，避免部分输出。 | Hadoop _SUCCESS文件、S3一致性模型  |
| **副本冗余**     | 关键数据多副本存储，预防节点故障。             | HDFS 3副本策略、RAID存储           |

###### 资源效率提升

| **优化方向**     | **具体措施**                                               | **工具/框架**                               |
| :--------------- | :--------------------------------------------------------- | :------------------------------------------ |
| **动态资源分配** | 根据负载自动扩缩容，空闲时释放资源。                       | Kubernetes HPA、YARN弹性调度                |
| **资源隔离**     | 通过容器化（Docker）或队列隔离（YARN队列）避免作业间干扰。 | YARN Capacity Scheduler、Mesos资源隔离      |
| **流批资源复用** | 共享集群运行批处理和流任务，提高资源利用率。               | Flink统一运行时、Spark Structured Streaming |

###### 存储与数据管理优化

| **优化方向**     | **具体措施**                                    | **技术方案**                                     |
| :--------------- | :---------------------------------------------- | :----------------------------------------------- |
| **列式存储**     | 按列压缩存储，提升扫描效率。                    | Parquet、ORC格式                                 |
| **数据分层存储** | 冷热数据分离，热数据存SSD，冷数据归档低成本存储 | HDFS分层存储、S3 Intelligent-Tiering             |
| **增量处理**     | 仅处理新增数据，避免全量重跑。                  | Hudi/Iceberg增量更新、CDC（Change Data Capture） |
| **数据压缩**     | 使用高效压缩算法减少存储与传输开销。            | Spark压缩配置、Kafka消息压缩                     |

#### 流批一体

###### Lambda 架构

- **批处理层**（Batch Layer）：处理全量历史数据（如Hadoop、Spark），生成精准结果。
- **速度层**（Speed Layer）：处理实时数据（如Flink、Kafka Streams），生成近似结果。
- **服务层**（Serving Layer）：合并批和流的结果（如HBase、Redis），供查询使用。

###### Kappa 架构

- **仅保留流处理层**：通过流处理引擎（如Flink）重放历史数据，替代批处理层。
- **依赖持久化事件日志**：如Kafka长期存储原始数据（支持全量回放）。

###### 流批一体的核心思想

- **统一的数据模型**

  - **批数据**：视为有界流（Bounded Stream），即流的一个有限子集。
  - **流数据**：视为无界流（Unbounded Stream），持续追加。

  - **统一处理语义**：无论数据来自历史还是实时，均通过 **事件时间（Event Time）** 和 **处理时间（Processing Time）** 统一管理。

- **统一的 API**
  - **声明式编程**：通过同一套API（如SQL、DataStream/DataSet API）描述批和流任务。
  - **代码复用**：业务逻辑（如聚合、过滤）无需为批和流分别实现。

- **统一的运行时引擎**
  - **共享执行引擎**：批和流任务在同一运行时中执行（如Flink的流式运行时支持批处理优化）。
  - **资源动态分配**：根据负载自动调配资源（如夜间跑批时分配更多资源）。

###### 流批一体的技术实现

-	**Apache Flink**

  - **流式优先，批是流的特例**：

    >  **流处理**：`DataStream API` 处理无界数据，支持事件时间、状态管理、精确一次语义。
    >
    > **批处理**：`DataSet API`（旧版）或直接使用`DataStream API`处理有界数据，自动优化执行计划。

  - **批处理优化**：对有界数据关闭冗余容错机制（如Checkpoint），减少开销。
  - **动态延迟调度**：批任务优先处理关键路径数据，缩短作业完成时间。

-	**Apache Spark（Structured Streaming）**

  -	**微批处理**：将流数据切分为小批次（如1秒窗口），复用批处理引擎。
  -	**连续处理模式**：实验性支持低至毫秒级的延迟（类似流处理）。

## 第四部分 流处理系统

#### 流处理系统架构

###### 流处理架构图

```
[数据源] → [数据接入层] → [流处理引擎] → [数据输出层]  
               ↑              ↓  
          [状态存储层]    [协调服务层]
```

###### 数据源（Source）

- **功能**：持续生成或传输数据流，作为系统的输入。
- **消息队列**：Kafka、RabbitMQ、AWS Kinesis（高吞吐、持久化）。
- **日志系统**：Fluentd、Logstash（实时采集应用日志）。
- **数据库CDC**：Debezium（捕获MySQL/Oracle变更日志）。

###### 数据接入层（Ingestion Layer）

- **功能**：将数据从源头可靠地摄入到处理引擎，支持反压（Backpressure）机制。
- **反压控制**：动态调整数据拉取速率，避免下游过载（如Kafka消费者自适应拉取）。
- **格式转换**：将原始数据（如JSON、Avro）转换为处理引擎内部格式。
- **分区策略**：按Key分区保证数据局部性（如用户ID哈希分区）。

###### 流处理引擎（Processing Engine）

- **核心功能**：执行实时计算逻辑，包括过滤、转换、聚合、关联、排序等。
- **算子（Operator）**：基础计算单元（如`Map`、`Filter`、`Window`）。
- **时间管理**：
  - **事件时间（Event Time）**：基于数据生成时间处理。
  - **水位线（Watermark）**：跟踪事件时间进度，处理乱序数据。
- **窗口（Window）**：滚动窗口、滑动窗口、会话窗口。
- **状态管理**：
  - **键控状态（Keyed State）**：与特定Key绑定（如用户会话计数）。
  - **算子状态（Operator State）**：与任务实例绑定（如Kafka消费偏移量）。

###### 状态存储层（State Storage）

- **功能**：持久化计算过程中的中间状态，支持容错与恢复。
- **存储类型**：
  - **本地状态**：内存或本地磁盘（如RocksDB），低延迟但易丢失。
  - **分布式状态**：HDFS、S3、分布式内存（如Ignite），高可靠但延迟较高。
- **容错机制**：
  - **检查点（Checkpoint）**：定期全量/增量快照（如Flink的异步屏障快照）。
  - **精确一次语义（Exactly-Once）**：通过分布式快照或事务写入实现。

###### 数据输出层（Sink）

- **功能**：将处理结果输出到外部系统。
- **数据库**：Redis（实时查询）、HBase（持久化存储）。
- **消息队列**：Kafka（二次处理）、Pulsar（多租户支持）。
- **BI工具**：Grafana（实时仪表盘）、Elasticsearch（日志分析）。
- **文件系统**：HDFS、S3（归档或供批处理使用）。

###### 协调服务层（Coordination Service）

- **功能**：管理集群元数据、任务调度与故障恢复。
- **分布式协调**：ZooKeeper、etcd（选举、配置管理）。
- **资源调度**：Kubernetes（容器编排）、YARN（资源分配）。
- **监控告警**：Prometheus（指标采集）、Grafana（可视化）。

#### 流处理核心技术

###### 时间语义（Time Semantics）

- **时间类型**
  - **事件时间（Event Time）**：数据实际发生的时间（如用户点击时间戳），需要处理乱序。
  - **处理时间（Processing Time）**：数据被系统处理的时间（如服务器接收时间），无需处理乱序。
  - **摄入时间（Ingestion Time）**：数据进入流处理系统的时间。

- **水位线（Watermark）**
  - **作用**：解决乱序数据问题，标记事件时间的进展。例如，水位线`T`表示所有事件时间 ≤ `T` 的数据**理论上**已到达。
  - **固定延迟**：`Watermark = 最大事件时间 - 固定延迟`（如允许数据迟到2秒）。
  - **自定义策略**：根据数据特征动态调整（如数据源分区独立生成水位线）。

###### 窗口机制（Windowing）

- **窗口类型**
  - **滚动窗口（Tumbling Window）**：固定大小、无重叠（如每5分钟统计一次），适用于周期性报表。
  - **滑动窗口（Sliding Window）**：固定大小、有重叠（如每1分钟统计过去5分钟的数据），适用于实时监控。
  - **会话窗口（Session Window）**：基于事件间隔动态划分（如用户两次操作间隔超过10分钟视为新会话）。
  - **全局窗口（Global Window）**：所有数据视为一个窗口，需自定义触发器（Trigger）决定何时输出结果。

- **窗口触发器（Trigger）**
  - **作用**：决定何时触发窗口计算（如基于时间、数据量或自定义条件）。
  - **时间驱动**：窗口结束时触发。
  - **数据量驱动**：累计一定数量数据后触发。
  - **事件时间驱动**：根据水位线触发。
  - **增量处理**：提前输出中间结果（如每分钟输出一次当前窗口值）。

- **迟到数据处理**
  - **丢弃**：忽略迟到数据（牺牲准确性）。
  - **侧输出（Side Output）**：将迟到数据路由到单独流，后续合并处理。
  - **允许延迟（Allowed Lateness）**：窗口关闭后仍接受迟到数据更新结果。

###### 状态管理（State Management）

- **状态类型**
  - **键控状态（Keyed State）**：与特定Key绑定（如每个用户的访问次数），只能用于Keyed Stream，本地存储并分区隔离。。
  - **算子状态（Operator State）**：与算子实例绑定（如Kafka消费偏移量），所有数据共享，分布式存储，适用于非Keyed场景。
  - **广播状态（Broadcast State）**：全局状态，所有任务实例共享（如规则配置），高吞吐更新，适合动态配置下发。。

- **状态后端（State Backend）**
  - **内存状态后端**：状态存于TaskManager堆内存，速度快但易丢失。
  - **RocksDB状态后端**：状态存于本地磁盘（RocksDB），支持增量检查点，容量大但速度较慢。
  - **分布式状态后端**：状态存于外部存储（如HDFS、S3），容错性强但延迟高。

###### 容错与处理语义（Fault Tolerance & Processing Semantics）

- **处理语义**

  | **语义**          | **定义**                       | **实现方式**                                 |
  | :---------------- | :----------------------------- | :------------------------------------------- |
  | **At-Most-Once**  | 数据可能丢失，但不会重复处理。 | 无重试机制，适合允许丢数据的场景。           |
  | **At-Least-Once** | 数据可能重复，但不会丢失。     | 简单重试（如Kafka消费者自动提交偏移）。      |
  | **Exactly-Once**  | 数据不丢不重，结果精确一次。   | 分布式快照（如Flink Checkpoint）+ 事务写入。 |

- **容错机制**

  - **检查点（Checkpoint）**：

    > **全量检查点**：定期保存完整状态（适用于小状态）。
    >
    > **增量检查点**：仅保存状态变化（如RocksDB的SST文件）。
    >
    > **分布式快照算法**：Chandy-Lamport算法（Flink使用）确保全局一致性。

  - **恢复流程**：

    > 1.从最近检查点恢复任务状态。
    >
    > 2.数据源重置到对应位置（如Kafka偏移量回滚）。
    >
    > 3.重新处理检查点后的数据。

- **两阶段提交（2PC）**
  - **场景**：输出到外部系统（如数据库）时保证精确一次。
  - **准备**：写入临时区域，锁定资源。
  - **提交**：检查点完成后提交数据。

###### 流式处理模型

- **数据处理模型**
  - **记录驱动（Record-at-a-Time）**：逐条处理数据，灵活性高但开销大（Apache Storm、早期Flink）。
  - **微批处理（Micro-Batch）**：将数据流切分为小批次处理，平衡延迟与吞吐（Spark Streaming）。
  - **事件驱动（Event-Driven）**：异步处理数据，低延迟但资源消耗高（Apache Flink、Kafka Streams）。

- **反压机制（Backpressure）**
  - **定义**：下游处理速度慢导致数据堆积。
  - **TCP反压**：通过网络缓冲区填满自然反压（如Flink的Credit-based机制）。
  - **动态速率调整**：根据下游处理能力动态调整数据摄入速率（如Kafka消费者自适应拉取）。

###### 流式SQL与复杂事件处理（CEP）

- **流式SQL**：通过标准SQL操作数据流，降低开发门槛。
- **复杂事件处理（CEP）**：检测数据流中的复杂模式（如连续登录失败3次）。

###### 主流流处理框架对比

| **框架**            | **核心特点**                                                 | **适用场景**                                |
| :------------------ | :----------------------------------------------------------- | :------------------------------------------ |
| **Apache Flink**    | 低延迟（毫秒级）、精确一次语义、流批一体、状态管理强大。     | 复杂事件处理、实时ETL、金融风控。           |
| **Kafka Streams**   | 轻量级库（无需集群）、与Kafka深度集成、仅支持精确一次语义。  | 微服务内实时处理、Kafka数据实时转换。       |
| **Apache Storm**    | 早期流处理框架、低延迟（微秒级）、容错较弱（至少一次语义）。 | 简单实时统计、监控告警（逐渐被Flink取代）。 |
| **Spark Streaming** | 微批处理（秒级延迟）、与Spark生态集成、适合准实时场景。      | 准实时报表、机器学习特征工程。              |
| **AWS Kinesis**     | 全托管服务、自动扩缩容、集成AWS生态（如Lambda、S3）。        | 云原生实时分析、IoT数据处理。               |

#### 流处理系统优化

###### 高吞吐与低延迟的平衡

- **业务场景**：
  - 数据持续高速生成（如IoT设备每秒百万条数据），系统需同时处理高吞吐量并维持毫秒级延迟。
  - 资源竞争导致处理速度下降，如CPU密集型计算阻塞数据摄入。
- **优化策略**：
  - **异步非阻塞处理**：将I/O操作异步化，避免线程阻塞（如Flink Async I/O）。
  - **流水线并行**：分解任务为多个并行阶段，提升吞吐（如Kafka分区并行消费）。
  - **内存优化**：使用堆外内存（Off-Heap）减少GC开销（如Flink RocksDB状态后端）。

###### 数据乱序与事件时间处理

- **业务场景**：
  - 网络延迟或分区传输导致数据到达顺序与事件时间（Event Time）不一致。
  - 窗口计算可能因迟到数据产生误差（如统计结果偏少）。
- **优化策略**：
  - **水位线（Watermark）机制**：动态跟踪事件时间进度，允许设定最大延迟容忍（如Flink的`allowedLateness`）。
  - **侧输出（Side Output）**：将迟到数据路由至旁路流，后续合并修正结果。

###### 状态管理与容错

- **业务场景**：
  - 长时间运行的流任务需维护TB级状态（如用户会话信息），故障恢复耗时。
  - 状态存储与访问效率影响整体性能。
- **优化策略**：
  - **增量检查点（Incremental Checkpoint）**：仅持久化状态变化，缩短快照时间（如RocksDB的SST文件增量快照）。
  - **状态分区与本地化**：按Key分片存储状态，确保任务重启后状态分布均衡（如Flink Key Groups）。
  - **状态TTL（Time-To-Live）**：自动清理过期状态（如7天前的用户会话）。

###### 资源动态扩展与利用率

- **业务场景**：
  - 流量波动大（如电商大促），固定资源分配导致高峰时资源不足，低峰时资源浪费。
  - 多租户环境下任务竞争资源，引发性能降级。
- **优化策略**：
  - **弹性扩缩容（Auto-scaling）**：根据负载动态调整TaskManager实例（如Kubernetes HPA）。
  - **资源隔离**：通过容器化（Docker）或队列（YARN Capacity Scheduler）隔离关键任务。
  - **批流混合部署**：利用流处理低峰期资源运行批处理任务（如夜间日志分析）。

###### 结果准确性与一致性

- **业务场景**：
  - 分布式环境下，网络分区或节点故障可能导致结果不一致（如重复计数或漏算）。
  - 精确一次（Exactly-Once）语义实现复杂度高。
- **优化策略**：
  - **分布式快照（Checkpoint）**：定期保存全局一致性状态（如Flink Chandy-Lamport算法）。
  - **端到端精确一次**：结合事务写入外部系统（如Kafka幂等生产者和事务API）。
  - **版本化状态（Versioned State）**：支持状态回滚与修正（如CDC场景下的数据版本合并）。