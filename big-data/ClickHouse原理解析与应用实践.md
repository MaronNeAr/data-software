## 第1章 ClickHouse概述

#### OLAP与ClickHouse的定位

###### OLAP的核心概念

- **OLTP**：服务于高并发、低延迟的短事务操作（如银行转账、订单支付），强调数据的增删改查（CRUD）和事务一致性（ACID）。
- **OLAP**：专注于大规模数据的复杂聚合分析（如统计报表多维分析），要求高吞吐/性能的批量查询，通常涉及全表扫描和多表关联。
- **OLAP的典型特征**：
  - **数据量大**：TB/PB级数据存储。
  - **查询复杂**：涉及GROUP BY、JOIN、窗口函数等聚合操作。
  - **读多写少**：数据批量写入，极少单行更新。
  - **高吞吐低延迟**：即使数据量极大，仍需快速响应分析请求。

###### ClickHouse的定位

- **列式存储的极致优化**

  - **列式存储**：数据按列而非行存储，同一列的数据类型相同，天然适合高压缩率（如LZ4、ZSTD算法），减少I/O和内存占用。

  - **向量化查询**：利用SIMD指令集（单指令多数据）对整列数据进行批量处理，大幅提升CPU利用率。

  - **数据预聚合**：通过MergeTree引擎的预排序（Primary Key）、跳数索引（Skipping Index）和物化视图（Materialized View）加速聚合查询。

- **分布式架构的天然支持**

  - **分片与副本**：数据可水平分片（Sharding）存储，副本（Replication）机制通过ZooKeeper协调实现高可用。

  - **分布式查询**：支持跨节点并行执行查询，自动路由和合并结果，用户无需感知分片细节。

- **实时分析能力**
  - **高写入吞吐**：通过类LSM-Tree（Log-Structured Merge-Tree）结构实现高效写入（如批量插入、后台合并）。
  
  - **准实时查询**：数据写入后通常在毫秒到秒级即可查询，适合实时报表和监控场景。
  
- **灵活性不足但性能优先**

  - **牺牲事务支持**：不支持ACID事务，不适合需要强一致性的OLTP场景。

  - **弱化单点更新**：单行更新（UPDATE/DELETE）效率低，需通过ALTER TABLE实现批量更新。

  - **复杂JOIN的局限**：大表JOIN性能较差，建议通过预计算或宽表规避。

###### ClickHouse与其他OLAP技术的对比

|   **技术/产品**   |           **核心优势**           |             **局限性**              |        **适用场景**        |
| :---------------: | :------------------------------: | :---------------------------------: | :------------------------: |
|  **ClickHouse**   | 极致查询性能、高压缩率、实时分析 |       JOIN能力弱、事务支持差        | 实时日志分析、用户行为分析 |
|  **Apache Hive**  |       生态完善、兼容Hadoop       | 延迟高（分钟级）、依赖MapReduce/Tez |    离线批处理、数据仓库    |
| **Apache Druid**  |    高并发低延迟、时间序列优化    |        架构复杂、存储成本高         |   实时监控、事件驱动分析   |
| **Elasticsearch** |       全文检索、近实时查询       |       聚合性能差、存储成本高        |     日志检索、文本分析     |
|   **Snowflake**   |    全托管、弹性扩展、多云支持    |       成本高昂、定制化能力弱        |        企业级云数仓        |

#### ClickHouse的诞生背景与核心优势

###### 诞生背景

- 初衷是为了解决其内部海量数据分析的瓶颈。
- 核心业务（如搜索引擎、广告系统、用户行为分析）需要实时处理PB级数据。
- 传统数据库（如MySQL）和Hadoop生态工具（如Hive）无法满足低延迟、高吞吐的分析需求。

###### 核心优势

- **列式存储与高效压缩**：数据按列存储，同类型数据紧密排列，压缩率极高（节省存储和I/O），适合聚合查询（仅读取相关列）。
- **向量化查询引擎**：利用CPU的SIMD指令并行处理整列数据，大幅提升计算效率（相比逐行处理）。
- **分布式架构**：天然支持分片和副本，数据可水平扩展，查询自动并行化，轻松应对PB级数据。
- **实时写入与查询**：写入吞吐高（百万级/秒），数据写入后毫秒级可查，支持实时分析场景（如监控、日志）。
- **高性能聚合计算**：预排序、跳数索引、物化视图等机制，复杂聚合（GROUP BY、窗口函数）比传统数据库快10-100倍。
- **易用性与SQL兼容**：支持标准SQL语法，学习成本低，且提供丰富扩展函数（如近似计算、地理数据处理）。
- **开源与社区生态**：完全开源，社区活跃，支持与Kafka、Hadoop、Spark等工具无缝集成。

#### ClickHouse的适用场景与局限性

###### ClickHouse的适用场景

- **实时日志分析**：Nginx访问日志、应用错误日志的实时聚合统计（PV/UV、错误率）。
- **用户行为分析**：用户点击流分析、漏斗转化计算、留存率统计。
- **时序数据存储与监控**：物联网（IoT）设备指标、服务器性能监控（Prometheus长期存储替代）。
- **大数据实时报表**：电商实时销售额看板、广告点击效果分析（支持TB级数据秒级响应）。
- **联机分析处理（OLAP）**：复杂聚合查询（GROUP BY、窗口函数）、多维数据分析。

###### ClickHouse的局限性

- **不适用于OLTP场景**：不支持事务（ACID）、高并发单行更新/删除效率极低。
- **不适合频繁单行更新/删除**：数据更新需通过批量ALTER TABLE实现，延迟高。
- **JOIN性能较弱**：大表JOIN容易成为性能瓶颈，建议预计算宽表或使用非规范化设计。
- **高并发点查询支持有限**：默认设计面向高吞吐分析查询，单点查询（如按主键查单行）性能不如MySQL/Redis。
- **存储成本与数据冷热分离**：全量数据存储成本较高，需自行管理冷热数据分层（如结合S3）。

## 第2章 快速入门

#### 数据类型与表引擎简介

###### 数据类型

- **基础类型**

  |   **类别**   |        **类型示例**        |                           **描述**                           |
  | :----------: | :------------------------: | :----------------------------------------------------------: |
  |   **整数**   |      `Int8`, `UInt32`      | 有符号/无符号整数（位数可选8/16/32/64），如`UInt32`表示0~4294967295。 |
  |  **浮点数**  |    `Float32`, `Float64`    |            单精度（32位）和双精度（64位）浮点数。            |
  |  **字符串**  | `String`, `FixedString(N)` | `String`存储任意长度文本，`FixedString(N)`定长字符串（适合枚举值，如国家代码）。 |
  | **日期时间** |     `Date`, `DateTime`     | `Date`精确到天（如`2023-10-01`），`DateTime`精确到秒（如`2023-10-01 10:00:00`）。 |
  |  **布尔值**  |           `Bool`           |    实际存储为`UInt8`（0或1），但支持`true`/`false`语法。     |
  |   **枚举**   |     `Enum8`, `Enum16`      | 预定义值的字符串映射（如`Enum8('success' = 1, 'fail' = 2)`），节省存储空间。 |

- **复合类型**

  |     **类型**      |                           **描述**                           |
  | :---------------: | :----------------------------------------------------------: |
  |   **Array(T)**    |     同类型元素数组（如`Array(String)`存储多个字符串）。      |
  | **Tuple(T1, T2)** | 异构元素的元组（如`Tuple(String, UInt32)`存储名称和年龄）。  |
  |    **Nested**     | 嵌套表结构（类似子表），需与`Array`结合使用（如`Nested(tag String, value Float64)`）。 |

- **特殊类型**

  |      **类型**      |                           **描述**                           |
  | :----------------: | :----------------------------------------------------------: |
  | **LowCardinality** | 低基数优化类型（如重复值多的字符串字段），自动压缩存储（类似字典编码）。 |
  | **Decimal(P, S)**  | 高精度浮点数（如`Decimal(18, 4)`表示小数点后4位，适合金融金额）。 |
  |      **UUID**      |       128位全局唯一标识符（如`UUID`类型存储设备ID）。        |
  |   **IPv4/IPv6**    | 优化存储IP地址（`IPv4`用`UInt32`，`IPv6`用`FixedString(16)`）。 |

###### 表引擎

- **基础引擎**

  |  **引擎**   |                           **特点**                           |
  | :---------: | :----------------------------------------------------------: |
  | **TinyLog** | 无索引，数据不压缩，适用于小表或临时测试（写入后不可修改）。 |
  |   **Log**   | 按列存储，支持并行查询，适合中等规模静态数据（写入后不可修改）。 |
  | **Memory**  | 数据仅存储在内存中，重启后丢失，适合高速缓存或中间结果暂存。 |

- **MergeTree引擎家族**

  |             **引擎**             |                           **特点**                           |
  | :------------------------------: | :----------------------------------------------------------: |
  |          **MergeTree**           | 基础引擎，支持主键索引、数据分区（PARTITION BY）和后台合并（Merge）。 |
  |      **ReplacingMergeTree**      | 在MergeTree基础上，自动去重相同排序键的数据（保留最后写入版本）。 |
  |       **SummingMergeTree**       |     自动合并时对数值列求和（如统计相同维度的指标累加）。     |
  |     **AggregatingMergeTree**     | 合并时预聚合数据，需配合`AggregateFunction`类型使用，适合实时OLAP。 |
  |     **CollapsingMergeTree**      |  通过标记字段（`sign`）合并时折叠数据（如删除旧版本记录）。  |
  | **VersionedCollapsingMergeTree** |     在折叠基础上支持版本控制（如按时间戳保留最新状态）。     |

- **集成引擎**

  |   **引擎**    |                           **特点**                           |
  | :-----------: | :----------------------------------------------------------: |
  |   **Kafka**   | 从Kafka主题消费数据（需指定Broker、Topic和格式），实时写入ClickHouse。 |
  |   **MySQL**   | 映射MySQL表到ClickHouse（类似外部表），支持读写操作（但性能不如原生表）。 |
  |   **HDFS**    |    直接读取HDFS文件（如Parquet/ORC格式），适合离线分析。     |
  | **JDBC/ODBC** |      通过JDBC或ODBC协议连接外部数据库（如PostgreSQL）。      |

###### 引擎选择

- **默认选择**：优先使用`MergeTree`系列引擎（90%场景适用）。
- **实时聚合**：`AggregatingMergeTree` + 物化视图实现预计算。
- **外部数据**：临时分析使用`MySQL`或`HDFS`引擎，长期存储建议导入本地表。
- **日志场景**：高吞吐写入选择`MergeTree`，小数据量测试用`TinyLog`。

## 第三章 核心架构设计

#### 列式存储原理与数据压缩

###### 列式存储原理

- ##### **列式存储 vs 行式存储**

  |   **特性**   |        **行式存储（如MySQL）**        |          **列式存储（ClickHouse）**          |
  | :----------: | :-----------------------------------: | :------------------------------------------: |
  | **数据排列** |     按行连续存储（所有字段相邻）      |       按列连续存储（单列数据紧密排列）       |
  | **适用场景** |     OLTP：频繁单行读写、事务操作      |      OLAP：批量读取、聚合计算、复杂分析      |
  | **I/O效率**  | 读取整行，即使只需少量字段（I/O浪费） |   仅读取查询涉及的列（减少I/O，提升吞吐）    |
  | **压缩潜力** |   低（不同数据类型相邻，压缩率低）    | 高（同列数据类型一致，重复模式多，压缩率高） |

- **列式存储的核心优势**

  - **减少I/O开销**：仅加载必要列数据，降低磁盘和网络带宽压力。

  - **向量化处理**：同列数据连续存储，可利用SIMD指令并行处理（如批量计算`SUM(price)`）。

  - **高效压缩**：同列数据重复性高，可采用针对性的压缩算法（如字典编码、Delta编码）。

  - **延迟物化**：在压缩数据上直接计算，减少内存占用（如聚合前不解压字符串）。

###### 数据压缩机制

- ##### **压缩算法选择**

  | **算法** | **压缩速度** | **解压速度** | **压缩率** |        **适用场景**        |
  | :------: | :----------: | :----------: | :--------: | :------------------------: |
  |   LZ4    |     极快     |     极快     |    中等    |     默认选择，通用场景     |
  |   ZSTD   |      快      |     极快     |     高     | 存储成本敏感，CPU资源充足  |
  |   ZLIB   |      慢      |     中等     |     高     | 历史兼容，不推荐新项目使用 |

- **压缩优化策略**
  - **自适应压缩级别**：ZSTD允许动态调整压缩级别（1-22），平衡速度与压缩率。
  - **列级压缩策略**：不同列可独立配置压缩算法（如文本列用ZSTD，数值列用LZ4）。
  - **字典编码**：对低基数列（如`country`枚举值）自动构建字典，用整数代替原始值存储。
  - **Delta压缩**：对有序数值列（如时间戳、自增ID），存储差值而非原始值，大幅减少存储空间。

#### 向量化执行引擎

###### 传统行式执行 vs 向量化执行

|    **特性**     |   **行式执行（Row-based）**    |      **向量化执行（Vectorized）**       |
| :-------------: | :----------------------------: | :-------------------------------------: |
|  **处理单位**   |      单行数据（逐行处理）      | 数据块（Block，包含多行数据的列式集合） |
| **CPU指令优化** | 频繁分支跳转，无法利用SIMD指令 |   按列连续处理，触发SIMD指令并行计算    |
| **缓存利用率**  |    低（随机访问不同列数据）    |  高（连续访问单列数据，缓存命中率高）   |
|  **适用场景**   |    OLTP（短事务、单行操作）    |       OLAP（批量计算、聚合分析）        |

- **行式执行**：逐行计算`price * quantity`，循环次数多，无法并行。
- **向量化执行**：将`price`和`quantity`两列数据分别加载到寄存器，通过SIMD指令批量计算。

###### ClickHouse的向量化实现

- ##### **数据块（Block）结构**

  - **处理单元**：每个Block包含多行数据（如默认8192行），按列存储（与内存中的列式布局一致）。
  - **内存连续**：单列数据在内存中连续排列，便于SIMD指令加载（如一次读取256位寄存器数据）。
  - **类型感知**：Block内数据类型明确（如`ColumnUInt32`），避免运行时类型判断开销。

- **SIMD指令加速**

  - SIMD（Single Instruction Multiple Data）：单条指令处理多个数据，例如：

    > AVX-512指令集：单指令处理16个`Int32`或8个`Double`。
    >
    > 适用操作：过滤（`WHERE`）、聚合（`SUM`）、数学运算（`sqrt`）、哈希计算等。

  - **自动向量化**：ClickHouse编译器（CLang）自动将循环展开为SIMD指令，或手动内联汇编优化关键路径。

- #####  **零拷贝与惰性物化**

  - **零拷贝（Zero-Copy）**：在Block之间传递数据时，仅传递指针而非复制数据，减少内存开销。
  - **惰性物化（Lazy Materialization）**:延迟生成中间结果，仅在必要时解压或转换数据。

###### 向量化执行的优势

- **性能提升**

  - **吞吐量**：相比行式引擎，向量化执行在聚合查询中可提升 **5-50倍** 性能。

  - **资源效率**：减少CPU指令分支预测失败和缓存失效，提高计算密度。

- **典型优化场景**

  - **聚合函数**：`sum`、`avg`、`count`等统计计算。

  - **过滤操作**：`WHERE`条件中的批量判断（如`WHERE value > 100`）。

  - **排序与JOIN**：向量化哈希表加速JOIN，SIMD加速排序比较。

###### 向量化执行的实现细节

- **列式内存布局**

  - **列数据连续存储**：每列数据在内存中连续分配，便于SIMD指令批量加载。

  - **数据类型优化**：使用紧凑数据类型（如`UInt8`代替`String`枚举）提升寄存器利用率。

- **查询计划优化**

  - **Pipeline执行模式**：将查询分解为多个Stage，每个Stage处理一个Block流，减少中间结果内存占用。

  - **并行流水线**：多个Pipeline并行执行，充分利用多核CPU（如`max_threads`参数控制并发度）。

- **JIT编译优化（实验性）**

  - **动态代码生成**：对热点循环（如聚合函数）生成机器码，绕过虚函数调用开销。

  - **LLVM优化**：使用LLVM编译器进一步优化生成的代码（需启用`compile_expressions`配置）。

#### 分布式计算模型（分片与副本机制）

###### 分片（Sharding）机制

- **目标**：将数据**水平拆分**到多个物理节点，突破单机存储与计算瓶颈。

- **分片键**：通过分片键（如用户ID、时间戳）决定数据分布策略，支持以下两种模式：

  - **显式分片**：用户手动指定数据分布规则。
  - **隐式分片**：通过哈希函数自动分配数据（如`rand()`随机分布）。

- ##### **分片策略**

  |  **分片类型**  |                         **实现方式**                         |          **适用场景**          |
  | :------------: | :----------------------------------------------------------: | :----------------------------: |
  |  **哈希分片**  |    对分片键（如`user_id`）计算哈希值，按模运算分配节点。     |    数据分布均匀，避免热点。    |
  |  **范围分片**  | 按分片键的范围（如时间范围`2023-01-01 ~ 2023-06-30`）分配节点。 |   时序数据，按时间分区查询。   |
  | **自定义分片** | 通过用户定义的规则（如`user_id % 10 < 5`）将数据映射到指定分片。 | 复杂业务逻辑（如多租户隔离）。 |

- **分片写入与查询**

  - **写入流程**：

    > 客户端向**任意节点**发送写入请求。
    >
    > 分布式表根据分片键计算目标分片节点。
    >
    > 数据路由到对应节点的本地表（如`local_table`）存储。

  - **查询流程**：

    > 查询发送到**协调节点**（任意节点）。
    >
    > 协调节点将查询分发到所有相关分片。
    >
    > 各分片并行执行查询，返回中间结果。
    >
    > 协调节点汇总结果并返回最终数据。

###### 副本（Replication）机制

- **副本的核心思想**

  - **目标**：在多个节点保存**相同数据副本**，实现**高可用**与**故障恢复**。

  - **实现方式**：基于**异步复制**，通过`ReplicatedMergeTree`引擎实现数据同步。

- **副本工作机制**

  - **依赖组件**：ZooKeeper（存储元数据与复制日志，协调副本间状态）。

  - **数据同步流程**：

    > 写入请求发送到**主副本**节点。
    >
    > 主副本将操作日志（如插入、合并）写入ZooKeeper。
    >
    > **从副本**监听ZooKeeper日志，拉取并应用变更。
    >
    > 副本间周期性校验数据一致性。

#### MergeTree引擎的底层逻辑

###### 数据写入与存储结构

- **数据写入流程**

  - **内存缓冲**：数据先写入内存缓冲区（`Memory`引擎的临时结构）。

  - **生成数据块（Part）**：缓冲区满或手动触发时，数据按列压缩并写入磁盘，形成独立的数据块（Part）。

  - **数据块特征**：

    > 每个Part包含数据文件（`.bin`）、索引文件（`.idx`）和元数据（`checksums.txt`）。
    >
    > Part按**写入顺序**命名（如`20231001_1_2_0`），命名规则为`{partition_id}_{min_block}_{max_block}_level`。

- **存储目录结构**

  ```bash
  # 示例表目录结构
  /user_behavior/
  └── 20231001_1_1_0/         # 数据Part
      ├── event_time.bin      # 列数据文件（压缩）
      ├── event_time.mrk2     # 列标记文件（定位数据位置）
      ├── primary.idx         # 主键索引文件
      └── checksums.txt       # 校验和元数据
  ```

###### 数据合并机制（Merge）

- **合并触发条件**

  - **后台线程周期性触发**：默认每10分钟检查一次可合并的Part。

  - **合并规则**：合并相邻的、时间范围重叠的Part；合并后生成更大的有序Part，减少碎片化。

- **合并过程**

  - **选择候选Part**：选取多个小Part（如`20231001_1_1_0`和`20231001_2_2_0`）。

  - **重新排序**：按主键（`ORDER BY`字段）重新排序数据，生成全局有序的新Part。

  - **更新元数据**：旧Part标记为过期，新Part生效（原子性操作）。

- **合并策略优化**

  - **层级合并（Leveled Merge）**：`level`值表示合并次数（如`20231001_1_2_1`中的`1`）；高层级Part优先合并，减少合并次数。

  - **TTL（Time to Live）**：自动删除过期数据（如保留最近30天数据）。

###### 主键索引与数据查询加速

- **主键（PRIMARY KEY）设计**

  - **主键定义**：通过`ORDER BY`字段隐式定义主键（默认与`ORDER BY`相同，可独立指定）。

  - **主键作用**：确定数据在磁盘上的物理排序顺序；生成稀疏索引（`.idx`文件），加速范围查询。

- **稀疏索引原理**

  - **索引粒度（index_granularity）**：默认每8192行生成一个索引条目。

  - **索引结构**：存储每个索引粒度的主键最小值（如`event_time`的最小值）。

  - **查询流程**：根据查询条件定位主键索引的起始和结束位置；跳过不满足条件的数据块（`Mark`文件定位具体列数据位置）。

###### 跳数索引（Skipping Index）

- **跳数索引的作用**

  - **辅助过滤**：在主键索引基础上，进一步加速非主键字段的查询（如`event_type`）。

  - **索引类型**：支持`minmax`、`set`、`bloom_filter`等算法。

- **索引工作原理**

  - **生成阶段**：在数据合并时生成跳数索引。

  - **查询阶段**：根据索引快速判断数据块是否包含目标值（如`event_type = 'click'`）。

###### 分区管理（PARTITION BY）

- **数据裁剪**：按分区字段（如时间）快速过滤无关数据块。

- **生命周期管理**：按分区删除过期数据（如`ALTER TABLE ... DROP PARTITION`）。

###### 最佳实践

- **主键设计**：选择高频查询字段，确保主键前缀能覆盖大多数查询条件；避免主键过多字段（1-3列），减少索引冗余。

- **分区策略**：按时间分区（如`PARTITION BY toYYYYMM(event_time)`）；单个分区数据量控制在10GB-100GB，避免分区过多。

- **跳数索引优化**：高基数字段（如`user_id`）使用`bloom_filter`索引；低基数字段（如`status`）使用`set`或`minmax`索引。

- **监控与维护**：定期检查`system.parts`表，合并小Part（`OPTIMIZE TABLE FINAL`）；定期清理过期数据。

## 第四章 存储引擎详解

#### MergeTree家族

|           **引擎名称**           |                         **核心功能**                         |        **典型应用场景**        |
| :------------------------------: | :----------------------------------------------------------: | :----------------------------: |
|          **MergeTree**           |         基础引擎，支持主键索引、数据合并和分区管理。         |        通用时序数据分析        |
|      **ReplacingMergeTree**      |        合并时按主键去重（保留最新或指定版本的数据）。        |     数据去重（如日志去重）     |
|       **SummingMergeTree**       |         合并时对数值列求和（预聚合相同主键的数据）。         |    指标累加（如PV/UV统计）     |
|     **AggregatingMergeTree**     | 合并时执行预定义的聚合函数（需配合`AggregateFunction`类型）。 |    复杂聚合（如留存率计算）    |
|     **CollapsingMergeTree**      |     通过标记字段（`sign`）折叠旧数据（如删除过时记录）。     | 数据版本管理（如状态变更记录） |
| **VersionedCollapsingMergeTree** |      支持按版本号折叠数据，避免乱序写入导致的数据错误。      | 高并发写入场景下的数据版本控制 |
|      **GraphiteMergeTree**       |       专为Graphite监控数据设计，支持自动聚合和降采样。       |        时序监控数据存储        |

#### Log家族

| **引擎名称**  |                        **核心特性**                        |           **适用场景**           |                 **限制**                 |
| :-----------: | :--------------------------------------------------------: | :------------------------------: | :--------------------------------------: |
|  **TinyLog**  |     数据按列存储，不支持并发读写，写入时直接追加数据。     |     小规模临时数据（<1万行）     | 不支持索引，性能低，数据损坏后不可恢复。 |
| **StripeLog** | 数据按行存储（类似日志文件），支持并发读写，适合流式写入。 | 日志类数据的快速写入（测试环境） |       查询效率低，不支持复杂操作。       |
|    **Log**    |     按列存储（类似TinyLog），支持并发读写和块级压缩。      |     小规模日志或中间结果存储     |      无索引，仅适合全表扫描类查询。      |

## 第五章 查询执行与优化

#### SQL语法扩展与执行计划分析

###### 特殊函数与子句

- **`WITH`子句**：定义临时表达式（CTE），复用中间结果。
  - `WITH tmp AS (SELECT ...) SELECT * FROM tmp`
- **`ANY`修饰符**：在`JOIN`时仅保留第一个匹配的行（避免笛卡尔积爆炸）。
  - `SELECT ... FROM table1 ANY LEFT JOIN table2 USING (key)`
- **`GLOBAL IN`/`GLOBAL JOIN`**：在分布式查询中，将右表广播到所有节点执行。
  - `SELECT ... WHERE id GLOBAL IN (SELECT id FROM remote_table)`
- **`LIMIT n BY expr`**：按`expr`分组后，每组取前`n`条数据（类似窗口函数`ROW_NUMBER`）。
  - `SELECT * FROM table LIMIT 3 BY date ORDER BY time DESC`
- **`SAMPLE`子句**：快速采样数据（基于哈希或分块）。
  - `SELECT * FROM table LIMIT 3 BY date ORDER BY time DESC`。
- **`ARRAY JOIN`**：展开数组或嵌套结构为多行。
  - `SELECT * FROM table ARRAY JOIN tags`

###### 聚合函数扩展

- **`uniqCombined`**：近似计算UV（误差率<1%），内存占用低。
  - `SELECT uniqCombined(user_id) FROM logs`
- **`quantileTDigest`**：近似计算分位数（如P99延迟）。
  - `SELECT quantileTDigest(0.99)(latency) FROM requests`
- **`sumMap`/`minMap`**：对Map类型（`Key-Value`）的聚合计算。
  - `SELECT sumMap(keys, values) FROM metrics`
- **`anyLast`**：返回最后一次出现的非NULL值（适合状态跟踪）。
  - `SELECT anyLast(status) FROM events GROUP BY user_id`

###### 执行计划优化策略

- **主键索引**：确保`WHERE`条件命中`ORDER BY`键的前缀。
- **跳数索引**：对高基数列使用`SET`或`MINMAX`索引加速过滤。
- **避免全表扫描**：使用分区裁剪（`PARTITION BY`）和索引过滤；避免在`WHERE`中对非索引列进行复杂计算。
- **聚合优化**
  - **预聚合**：使用`SummingMergeTree`或`AggregatingMergeTree`减少实时计算。
  - **Combiner优化**：在分布式查询中，优先在本地节点预聚合。

- **JOIN优化**
  - **`ANY JOIN`**：右表存在重复键，只需保留第一个匹配值。
  - **`GLOBAL JOIN`**：右表数据量小，广播到所有节点避免Shuffle。
  - **`DISTRIBUTED JOIN`**：右表数据量大，按Join键分布数据。

- **资源控制**：设置内存限制`max_memory_usage`；调整线程数`max_threads`、`background_pool_size`。

#### 索引原理

###### 索引类型与适用场景

- **主键索引**：基于排序键（`ORDER BY`）的粗粒度索引，定位数据块（Granule）范围，如时间范围过滤。
- **跳数索引**：基于列值统计的细粒度索引，跳过不满足条件的数据块，如高基数列的等值或范围过滤。

###### 主键索引（Primary Key）

- **数据排序**：MergeTree表的数据按`ORDER BY`指定的列严格排序后存储在磁盘，形成​**​有序数据块（Granule）​**​默认包含`8192`行数据。
- **索引结构**：主键索引记录每个Granule中​**​第一个行的排序键值​**​，并持久化为`primary.idx`文件。索引条目与Granule一一对应。
- **工作流程**
  - **查询条件解析**：优化器提取`WHERE`条件中涉及主键的过滤表达式（如`date >= '2023-01-01'`）。
  - **索引匹配**：通过二分查找定位到满足条件的Granule范围。
  - **数据读取**：仅加载相关Granule的数据文件（`.bin`），跳过无关数据块。

- **优化策略**
  - **前缀匹配原则**：主键索引仅对`ORDER BY`键的​**​前缀列​**​有效。若查询条件不包含前缀列，索引将失效。
  - **合理选择排序键**：将高频过滤条件（如时间列）放在`ORDER BY`最左侧。

###### 跳数索引（Data Skipping Indexes）

- **数据块级统计**：跳数索引为每个Granule生成统计信息（如最大/小值），记录在`skp_idx_[index_name].idx`和`.mrk`文件中。
- **跳过机制**：查询时根据索引统计信息，跳过不满足条件的Granule，减少数据读取量。
- **索引类型**
  - **minmax**：记录Granule中列的最小值/最大值，适用于范围查询（如数值列、日期列）。
  - **set(max_rows)**：记录Granule中列的取值集合（最多`max_rows`个），适用于低基数枚举列（如状态码）。
  - **bloom_filter**：基于布隆过滤器判断列值是否存在，适用于高基数列的等值查询（如UserID）。
  - **ngrambf_v1**：支持文本子串的布隆过滤器，适用于模糊查询（如`LIKE '%error%'`）。
  - **tokenbf_v1**：基于分词后的布隆过滤器，适用于分词后的文本搜索（如日志关键词）。

###### 索引的存储与维护

- **存储结构**

  - **主键索引**：每个MergeTree表对应一个`primary.idx`文件，按Granule顺序存储排序键的起始值。

  - **跳数索引**：

    > 1.`.idx`：存储索引数据（如minmax值、布隆过滤器位数组）。
    >
    > 2.`.mrk`：记录索引条目与数据文件的偏移量映射。

- **数据写入与合并**
  - **写入阶段**：数据插入时，每个新生成的Part（数据部分）独立维护其索引文件。
  - **合并阶段（Merge）**：多个Parts合并时，重新生成合并后的索引文件，确保索引的连续性。

###### 索引优化策略

- **主键设计原则**
  - **高频过滤列前置**：`ORDER BY`的第一列应为最常用的范围过滤条件（如时间列）
  - **避免过长主键**：主键列过多会增加索引文件大小，降低查询效率。

- **跳数索引选择**

  - **低基数列** → `set`或`minmax`索引。

  - **高基数等值查询** → `bloom_filter`。

  - **文本搜索** → `ngrambf_v1`或`tokenbf_v1`。

- **参数调优**
  - **index_granularity**：减小此值（如`4096`）可提升索引精度，但会增加索引文件大小。
  - **GRANULARITY**：跳数索引的粒度需平衡精度与存储开销。较小的粒度（如`2`）过滤更精确，但索引条目更多。

#### 查询优化的核心策略

###### 执行计划分析与优化

- **执行计划分析**

  - **`ReadFromMergeTree`**：检查是否命中主键索引或跳数索引（如显示`Index ... used`）。

  - **`Aggregating`**：判断是否启用并行聚合（`ParallelAggregating`）。

  - **`Sorting`**：是否使用外部排序（`ExternalSort`）或内存排序（`InMemorySort`）。

- **执行优化**

  - **并行化**：设置`parallel_aggregation_min_rows`（默认`100000`）触发多线程聚合。

  - **索引覆盖**：确保`WHERE`条件匹配主键前缀或跳数索引。

###### 查询优化

- **过滤条件下推**：将过滤条件尽可能提前，减少中间结果集。
- **ClickHouse语法扩展**：LIMIT BY替代窗口函数、ANY JOIN减少Shuffle。
- **分布式查询优化**
  - **GLOBAL修饰符**：小表右连接时使用`GLOBAL IN`或`GLOBAL JOIN`，避免重复计算。
  - **本地预聚合**：分布式聚合前启用`distributed_aggregation_memory_efficient`。

- **数据预聚合与存储优化**
  - **物化视图加速**：预计算高频指标；直接查询物化视图，避免全量扫描
  - **冷热数据分层**：按时间自动迁移冷数据到廉价存储（TTL策略）。

###### 分布式场景优化

- **分片键选择**
  - **均匀分布**：选择高基数列（如`user_id`）作为分片键，避免数据倾斜。
  - **本地性优化**：按业务属性分片（如`region`），减少跨节点查询。
- **副本与一致性**
  - **异步复制**：使用`ReplicatedMergeTree`实现副本同步，设置`max_replicated_queries`限制并发。
  - **最终一致性**：查询时添加`SET allow_experimental_consistency=1`容忍副本延迟。

#### 资源管理与并发控制

###### 资源管理：核心参数与策略

- **内存管理**
  - `max_memory_usage`：单个查询最大内存使用量（默认10GB，推荐根据节点内存调整）。
  - `max_bytes_before_external_sort`：触发外部排序的阈值，超过后使用磁盘暂存数据（默认1GB，推荐设为总内存的50%）。
  - `max_memory_usage_for_all_queries`：所有查询的总内存限制（默认0，表示无限制，推荐设为物理内存的80%）。
  - `memory_overcommit_ratio`：允许内存超卖的比例（默认0，不允许超卖）。

- **CPU管理**
  - `max_threads`：单查询最大线程数（默认物理CPU核数，推荐设为逻辑CPU核数的75%）。
  - `background_pool_size`：后台任务（合并、删除）的线程池大小，推荐生产环境设为`16`。
  - `load_balancing`：分布式查询的分片调度策略（随机/就近优先）。

- **磁盘与网络**
  - `max_concurrent_queries`：最大默认并发查询数（默认100），推荐根据磁盘IOPS进行调整。
  - `max_bytes_to_read`：单查询最大读取数据量（默认0，无限制），需要限制大查询。
  - `network_bandwidth`：网络带宽限制（默认0，无限制），推荐网络带宽限制（默认0，无限制）。

###### 并发控制：队列与优先级s

- **查询队列机制**

  - **`max_running_queries`**：同时运行的查询数（默认100），超出后新查询排队。

  - **`max_waiting_queries`**：最大等待查询数（默认1000），超出后拒绝新查询。

- **优先级调度**：通过**`priority`**参数设置查询优先级（范围1-10，默认0）。
- **资源组（Resource Groups）**：配置资源组，实现物理资源隔离。

## 第六章 分布式与高可用

#### 集群配置与ZooKeeper依赖

###### ClickHouse集群基础

- **集群架构**

  - **分片（Shard）**：水平分割数据，每个分片存储部分数据（如按用户ID哈希分片）。

  - **副本（Replica）**：同一分片的多个副本，保障数据高可用（如2副本互为备份）。

- **集群配置核心文件**

  - **`config.xml`**：全局配置，定义集群拓扑和ZooKeeper连接。

  - **`metrika.xml`**（或内嵌在`config.xml`）：定义集群名称、分片和副本信息。

###### 集群配置实战

- **定义集群拓扑**

  ```xml
  <remote_servers>
      <my_cluster> <!-- 集群名称 -->
          <shard> <!-- 分片1（包含2副本） -->
              <replica>
                  <host>node1</host>
                  <port>9000</port>
              </replica>
          </shard>
      </my_cluster>
  </remote_servers>
  ```

- ##### **分布式表**

  - **普通分布式表**：数据分布在分片，无需ZooKeeper。
  - **复制表（ReplicatedMergeTree）**：依赖ZooKeeper同步副本数据。

###### ZooKeeper的核心作用

- **元数据管理**

  - **Replicated表的元数据**：存储表结构、分区信息、副本状态等。

  - **操作日志（Log）**：记录数据插入、合并、删除等操作，确保副本间一致性。

- **副本协调**

  - **Leader选举**：副本通过ZooKeeper选举Leader，协调数据写入顺序。

  - **数据同步**：副本通过监听ZooKeeper节点变化，拉取缺失数据块。

  - **分布式锁**：在DDL操作（如`ALTER`、`OPTIMIZE`）时加锁，避免多副本冲突。

- **监控与恢复**

  - **健康检查**：ZooKeeper检测副本在线状态，触发故障转移。

  - **数据恢复**：副本离线后，通过ZooKeeper日志重新同步数据。

#### 数据分片与副本同步机制

###### 数据分片（Sharding）

- **分片的核心作用**

  - **水平扩展**：将数据分散到多个节点，突破单机存储与计算瓶颈。

  - **负载均衡**：通过分片键（Sharding Key）均匀分布数据，避免热点。

  - **并行查询**：分布式表（`Distributed`引擎）将查询分发到各分片，合并结果。

- **分片策略**
  - **随机分片**：使用`rand()`函数随机分配数据到分片。
  - **哈希分片**：对分片键（如`user_id`）进行哈希计算，确定目标分片。
- **分片键选择原则**
  - **高基数**：确保数据均匀分布（如用户ID、订单ID）。
  - **查询匹配**：优先选择高频过滤条件作为分片键，利用分片剪裁（Shard Pruning）。
  - **避免倾斜**：监控`system.tables`的`part_count`，防止分片数据不均。

###### 副本（Replication）

- **副本的核心作用**
  - **高可用性**：单分片多副本，避免数据丢失或服务中断。
  - **读扩展**：副本间负载均衡，提升查询吞吐量。
  - **容灾恢复**：故障副本重启后自动同步数据。

- **副本同步机制**
  - **写入领导者（Leader）**：客户端向Leader副本写入数据。
  - **记录操作日志**：Leader将操作日志（如INSERT、MERGE）写入ZooKeeper。
  - **副本拉取日志**：Follower副本监听ZooKeeper，获取日志并重放操作。
  - **数据同步**：Follower从Leader或其他副本拉取缺失的数据块（Part）。

#### 分布式查询的负载均衡与容错

###### 负载均衡机制

- **查询分发策略**
  - **随机分发**：将查询请求随机发送到分片或副本节点。
  - **轮询（Round-Robin）**：按顺序依次选择分片或副本节点。
  - **就近优先（Nearest）**：根据网络拓扑或延迟选择最近的副本节点（需配置`network_weights`）。
  - **权重分发（Weighted）**：根据节点权重分配请求（通过`weight`参数配置）。
- **副本选择逻辑**
  - **Leader优先**：默认优先选择Leader副本（写入节点），确保数据一致性。
  - **健康检查**：自动跳过不可用副本（通过`max_replica_delay_for_distributed_queries`限制副本延迟）。

###### 容错机制

- **故障检测与重试**
  - **连接超时**：通过`connect_timeout`和`receive_timeout`控制节点响应等待时间。
  - **自动重试**：对失败请求自动重试其他副本或分片（默认重试3次）。

- **副本切换流程**
  - **检测故障**：ZooKeeper监控副本心跳，标记节点为不可用。
  - **切换副本**：查询自动切换到其他可用副本。
  - **数据补偿**：故障期间写入的数据通过ZooKeeper日志同步恢复。

###### 分布式查询执行流程

- **客户端发起查询**：向协调节点（任意ClickHouse节点）发送请求。
- **生成分片任务**：根据WHERE条件剪裁分片（Shard Pruning），减少不必要的请求。
- **分发子查询**：将查询发送到相关分片的所有副本。
- **本地执行与聚合**：各分片在本地执行查询，返回中间结果。
- **全局聚合**：协调节点汇总结果并返回客户端。