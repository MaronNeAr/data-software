## 模块一：线性代数（核心基础）

#### 向量与矩阵运算

###### 向量（Vector）

- **向量**：有序的一维数组，表示空间中的方向和大小的量。
  - 示例：用户特征向量 `[年龄=25, 性别=1, 购买次数=5]`
  - 数学表示：$v=[v1,v2,…,v^n]^T∈R^n$
- **基本运算**：
  - **加减法**：逐元素相加减，$u+v=[u_1+v_1,u_2+v_2,…,u_n+v_n]$
  - **标量乘法**：$k⋅v=[kv_1,kv_2,…,kv_n]$
- **几何意义**：向量可以表示数据点（如用户画像）、特征（如图像像素值）、模型参数（如神经网络权重）。
- **AI应用场景**
  - **特征表示**：用户画像、文本词向量（Word2Vec）、图像特征向量（ResNet输出）。
  - **相似度计算**：余弦相似度：$cos⁡θ=u⋅v∥u∥∥v∥$（用于推荐系统）。
  - **梯度下降**：模型参数更新：$w:=w−α∇J(w)$，其中 $∇J(w)$是梯度向量。

###### 矩阵（Matrix）

- **矩阵**：二维数组，用于表示线性变换、数据表或批量数据。

  - 示例：用户-商品评分矩阵（推荐系统）、图像像素矩阵（CNN输入）。
  - 数学表示：$A∈R^{m×n}$，元素 $a_{ij}$ 表示第 $i$行第 $j$ 列的值。

- **核心运算**：

  - **矩阵加法**：同维矩阵逐元素相加。
  - **矩阵乘法**：$C=A⋅B$，其中 $c_{ij}=\sum_{k=1}^{n}a_{ik}b_{kj}$（前行后列点积）。
  - **转置**：$A^T$，行列互换。
  - **逆矩阵**：$A^{−1}$（若存在），满足 $A⋅A^{−1}=I$（单位矩阵）。
  - **迹（Trace）**：矩阵主对角线元素之和，$tr(A)=∑_{i=1}^na_{ii}$。

- ##### **几何意义**

  - **线性变换**：矩阵乘法可表示旋转、缩放、投影等几何变换。

  - **数据批量处理**：批量输入数据（如100张图片）可表示为三维张量（batch_size × height × width）。

- **AI应用场景**
  - **神经网络全连接层**：$Y=X⋅W+b$，其中$X$ 是输入矩阵，$W$ 是权重矩阵。
  - **推荐系统**：矩阵分解（SVD）：用户-商品评分矩阵 $R≈U⋅VT$，预测缺失评分。
  - **图像处理**：卷积操作：滤波器矩阵与图像局部区域做点积（CNN核心）。
  - **模型参数优化**：二阶优化方法（如牛顿法）需要计算海森矩阵（Hessian Matrix）。

#### 矩阵分解

###### 特征值分解（Eigen Decomposition）

- **适用条件**：仅适用于方阵（$A∈R_{n×n}$）且可对角化。

- **分解形式**：

  - $A=PΛP^{−1}$

  - $P$：特征向量矩阵（列向量为特征向量）
  - $Λ$：对角矩阵（对角线元素为特征值 $λ_1,λ_2,…,λ_n$）

- **几何意义**：将线性变换分解为 **旋转**（特征向量方向） + **缩放**（特征值幅度）。

- **AI应用场景**
  - **主成分分析（PCA）**：通过协方差矩阵的特征值分解，找到数据最大方差方向（降维）。
  - **PageRank算法**：网页链接矩阵的特征向量表示网页重要性排序。
  - **振动分析**：物理系统（如桥梁）的振动模式由刚度矩阵的特征值决定。

###### 奇异值分解（SVD, Singular Value Decomposition）

- **适用性**：任意矩阵（$A∈R_{m×n}$），包括非方阵和非对称矩阵。
- **分解形式**：$A=UΣV^T$
  - $U∈R_{m×m}$：左奇异向量矩阵（列向量正交）
  - $Σ∈R_{m×n}Σ$：对角矩阵（奇异值 $σ_1≥σ_2≥⋯≥σ_r>0$）
  - $V∈R_{n×n}$：右奇异向量矩阵（列向量正交）
- **几何意义**：将任意线性变换分解为 **旋转**（$V^T$）→ **缩放**（$Σ$）→ **旋转**（$U$）。

- **AI应用场景**
  - **推荐系统**：用户-商品评分矩阵 $R$的SVD分解：$R≈U_kΣ_kV_k^T$，预测缺失评分（Netflix Prize核心方法）。
  - **图像压缩**：保留前 $k$个奇异值（低秩近似），减少存储（如用10%的奇异值恢复90%的图像信息）。
  - **自然语言处理**：潜在语义分析（LSA）：词-文档矩阵SVD提取主题特征。

###### 主成分分析（PCA）

- **本质**：协方差矩阵的特征值分解（或数据矩阵的SVD）。

- **步骤**：

  - 数据标准化（均值归零）。

  - 计算协方差矩阵 $C=\frac1nX^TX$。

  - 对 $C$ 做特征值分解，取前 $k$大特征值对应的特征向量（主成分）。

  - 数据投影到主成分方向：$Y=X⋅W_k$（$W_k$为主成分矩阵）。

- **AI应用场景**
  - **数据可视化**：高维数据降维至2D/3D（如MNIST手写数字可视化）。
  - **去噪与特征提取**：去除低方差方向噪声（如人脸识别中的光照变化）。
  - **模型加速**：减少输入特征维度，提升训练效率（如金融风控中的高维特征降维）。

###### 工程实践要点

- **数值稳定性**：避免直接计算协方差矩阵（大矩阵易内存溢出），优先使用SVD（如$X = UΣV^T$，则主成分可直接取 $V$ 的前 $k$ 列）。
- **分布式计算**：大规模数据（如TB级）需用Spark MLlib的`RowMatrix.computePrincipalComponents`。
- **GPU加速**：使用CUDA加速的库（如cuSOLVER）加速SVD分解。
- **内存优化**：稀疏矩阵（如推荐系统中的用户-物品矩阵）采用压缩存储（CSR/CSC格式）。

#### 张量基础



## 模块二：概率与统计（建模与推断）

#### 概率基础



#### 统计推断



#### 随机过程



## 模块三：微积分（优化与建模）

#### 微分与导数



#### 积分与逼近



#### 优化方法



## 模块四：信息论（数据与模型压缩）

#### 信息度量



#### 编码理论



## 模块五：数值计算（工程化落地关键）

#### 数值稳定性



#### 迭代算法



#### 稀疏性与正则化